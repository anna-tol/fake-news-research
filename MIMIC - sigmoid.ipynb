{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import English language model\n",
    "nlp = spacy.load(\"/opt/anaconda3/lib/python3.8/site-packages/en_core_web_sm/en_core_web_sm-2.3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_table(\"data.csv\", delimiter = \",\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only what we need\n",
    "df = df[['label', 'statement', 'speaker', 'subject', 'context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>speaker</th>\n",
       "      <th>subject</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>abortion</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>health-care</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                          statement  \\\n",
       "0        false  Says the Annies List political group supports ...   \n",
       "1    half-true  When did the decline of coal start? It started...   \n",
       "2  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3        false  Health care reform legislation is likely to ma...   \n",
       "4    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "          speaker                             subject              context  \n",
       "0    dwayne-bohac                            abortion             a mailer  \n",
       "1  scott-surovell  energy,history,job-accomplishments      a floor speech.  \n",
       "2    barack-obama                      foreign-policy               Denver  \n",
       "3    blog-posting                         health-care       a news release  \n",
       "4   charlie-crist                        economy,jobs  an interview on CNN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels to binary (int)\n",
    "df['label'] = df['label'].replace(['mostly-true','half-true', 'barely-true', 'no-flip', 'half-flip', 'true'], 1)\n",
    "df['label'] = df['label'].replace(['mostly-false','pants-fire', 'full-flop', 'false'], 0)\n",
    "df['label'] = df['label'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>statement</th>\n",
       "      <th>speaker</th>\n",
       "      <th>subject</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>abortion</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>health-care</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                          statement         speaker  \\\n",
       "0      0  Says the Annies List political group supports ...    dwayne-bohac   \n",
       "1      1  When did the decline of coal start? It started...  scott-surovell   \n",
       "2      1  Hillary Clinton agrees with John McCain \"by vo...    barack-obama   \n",
       "3      0  Health care reform legislation is likely to ma...    blog-posting   \n",
       "4      1  The economic turnaround started at the end of ...   charlie-crist   \n",
       "\n",
       "                              subject              context  \n",
       "0                            abortion             a mailer  \n",
       "1  energy,history,job-accomplishments      a floor speech.  \n",
       "2                      foreign-policy               Denver  \n",
       "3                         health-care       a news release  \n",
       "4                        economy,jobs  an interview on CNN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing statement and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove extra whitespaces\n",
    "* Convert accented characters to ASCII characters\n",
    "* Expand contractions\n",
    "* Expand abbreviations\n",
    "* Remove special characters\n",
    "* Remove numbers\n",
    "* Lowercase all texts\n",
    "* Remove stopwords\n",
    "* Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extra whitespaces\n",
    "df['statement'] = [' '.join(statement.split()) for statement in df['statement']]\n",
    "df['context'] = [' '.join(context.split()) for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert accented characters to ASCII characters\n",
    "df['statement'] = [unidecode(statement) for statement in df['statement']]\n",
    "df['context'] = [unidecode(context) for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \n",
    "    flags = re.IGNORECASE | re.MULTILINE\n",
    "    \n",
    "    text = re.sub(r'`', \"'\", text, flags = flags)\n",
    "    text = re.sub(r\"(\\s|^)'(aight|cause)(\\s|$)\", '\\g<1>\\g<2>\\g<3>', text, flags = flags)\n",
    "    text = re.sub(r\"(\\s|^)'t(was|is)(\\s|$)\", r'\\g<1>it \\g<2>\\g<3>', text, flags = flags)\n",
    "    text = re.sub(r\"(\\s|^)ol'(\\s|$)\", '\\g<1>old\\g<2>', text, flags = flags)\n",
    "    \n",
    "    ## expand words without\n",
    "    text = re.sub(r\"\\b(aight)\\b\", 'alright', text, flags = flags)\n",
    "    text = re.sub(r'\\bcause\\b', 'because', text, flags = flags)\n",
    "    text = re.sub(r'\\b(finna|gonna)\\b', 'going to', text, flags = flags)\n",
    "    text = re.sub(r'\\bgimme\\b', 'give me', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgive'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhowdy\\b\", 'how do you do', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgotta\\b\", 'got to', text, flags = flags)\n",
    "    text = re.sub(r\"\\binnit\\b\", 'is it not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(can)(not)\\b\", r'\\g<1> \\g<2>', text, flags = flags)\n",
    "    text = re.sub(r\"\\bwanna\\b\", 'want to', text, flags = flags)\n",
    "    text = re.sub(r\"\\bmethinks\\b\", 'me thinks', text, flags = flags)\n",
    "    \n",
    "    ## one offs\n",
    "    text = re.sub(r\"\\bo'er\\b\", r'over', text, flags = flags)\n",
    "    text = re.sub(r\"\\bne'er\\b\", r'never', text, flags = flags)\n",
    "    text = re.sub(r\"\\bo'?clock\\b\", 'of the clock', text, flags = flags)\n",
    "    text = re.sub(r\"\\bma'am\\b\", 'madam', text, flags = flags)\n",
    "    text = re.sub(r\"\\bgiv'n\\b\", 'given', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\be'er\\b\", 'ever', text, flags = flags)\n",
    "    text = re.sub(r\"\\bd'ye\\b\", 'do you', text, flags = flags)\n",
    "    text = re.sub(r\"\\bg'?day\\b\", 'good day', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(ain|amn)'?t\\b\", 'am not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|can)'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(let)'?s\\b\", r'\\g<1> us', text, flags = flags)\n",
    "    \n",
    "    ## major expansions involving smaller\n",
    "    text = re.sub(r\"\\by'all'dn't've'd\\b\", 'you all would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all're\\b\", 'you all are', text, flags = flags)\n",
    "    text = re.sub(r\"\\by'all'd've\\b\", 'you all would have', text, flags = flags)\n",
    "    text = re.sub(r\"(\\s)y'all(\\s)\", r'\\g<1>you all\\g<2>', text, flags = flags)\n",
    "    \n",
    "    ## minor\n",
    "    text = re.sub(r\"\\b(won)'?t\\b\", 'will not', text, flags = flags)\n",
    "    text = re.sub(r\"\\bhe'd\\b\", 'he had', text, flags = flags)\n",
    "\n",
    "    ## major\n",
    "    text = re.sub(r\"\\b(I|we|who)'?d'?ve\\b\", r'\\g<1> would have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|would|must|should|would)n'?t'?ve\\b\", r'\\g<1> not have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he)'?dn'?t'?ve'?d\\b\", r'\\g<1> would not have had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(daren|daresn|dasn)'?t\", 'dare not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(he|how|i|it|she|that|there|these|they|we|what|where|which|who|you)'?ll\\b\", r'\\g<1> will', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(everybody|everyone|he|how|it|she|somebody|someone|something|that|there|this|what|when|where|which|who|why)'?s\\b\", r'\\g<1> is', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'a\\b\", r'\\g<1> am about to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m'o\\b\", r'\\g<1> am going to', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I)'?m\\b\", r'\\g<1> am', text, flags = flags)\n",
    "    text = re.sub(r\"\\bshan't\\b\", 'shall not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(are|could|did|does|do|go|had|has|have|is|may|might|must|need|ought|shall|should|was|were|would)n'?t\\b\", r'\\g<1> not', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(could|had|he|i|may|might|must|should|these|they|those|to|we|what|where|which|who|would|you)'?ve\\b\", r'\\g<1> have', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|so|that|there|these|they|those|we|what|where|which|who|why|you)'?re\\b\", r'\\g<1> are', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(I|it|she|that|there|they|we|which|you)'?d\\b\", r'\\g<1> had', text, flags = flags)\n",
    "    text = re.sub(r\"\\b(how|what|where|who|why)'?d\\b\", r'\\g<1> did', text, flags = flags)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['statement'] = [expand_contractions(statement) for statement in df['statement']]\n",
    "df['context'] = [expand_contractions(context) for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand abbreviations\n",
    "\n",
    "abbr = {'u.n.': 'united nations',\n",
    "        'a.m.': 'before midday',\n",
    "        'n.y.': 'new york',\n",
    "        'e.u.': 'european union',\n",
    "        'u.s.': 'united states',\n",
    "        'u.k.': 'united kingdom',\n",
    "        'd.c.': 'district columbia',\n",
    "        'a.k.a.': 'also known as',\n",
    "        'r.i.p.': 'rest in peace',\n",
    "        'n.h.': 'new hampshire',\n",
    "        'r.i.': 'rhode island',\n",
    "        's.c.': 'south carolina',\n",
    "        \n",
    "        'gop': 'the republican party',\n",
    "        'usa': 'united states of america',\n",
    "        'nato': 'north atlantic treaty organization',\n",
    "        'epa': 'environmental protection agency',\n",
    "                 \n",
    "        'rep.': 'representative',\n",
    "        'reps.': 'representatives',\n",
    "        'dem.': 'democrat',\n",
    "        'tenn.': 'tennessee',\n",
    "        'capt': 'captain',\n",
    "        'gov.': 'government',\n",
    "        'sen.': 'senator',\n",
    "        'mr.': 'mister',\n",
    "        'ok': 'okay',\n",
    "        'gen.': 'general',\n",
    "        'jr.': 'junior',\n",
    "       }\n",
    "\n",
    "def expand_abbreviations(text):\n",
    "\n",
    "    for key, value in abbr.items():\n",
    "        if key in text:\n",
    "            text = text.replace(key, value)\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['statement'] = [expand_abbreviations(statement) for statement in df['statement']]\n",
    "df['context'] = [expand_abbreviations(context) for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove special characters\n",
    "df['statement'] = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', statement) for statement in df['statement']]\n",
    "df['context'] = [re.sub(r\"[^a-zA-Z0-9]+\", ' ', context) for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "df['statement'] = [re.sub(\" \\d+\", \" \", statement) for statement in df['statement']]\n",
    "df['context'] = [re.sub(\" \\d+\", \" \", context) for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase all texts\n",
    "df['statement'] = [statement.lower() for statement in df['statement']]\n",
    "df['context'] = [context.lower() for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "df['statement'] = [\" \".join([token.lemma_ for token in nlp(statement)]) for statement in df['statement']]\n",
    "df['context'] = [\" \".join([token.lemma_ for token in nlp(context)]) for context in df['context']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing speaker and subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker: replace hyphen with space\n",
    "df['speaker'] = [speaker.replace(\"-\", \" \") for speaker in df['speaker']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject: replace hyphen and comma with space\n",
    "df['subject'] = [subject.replace(\"-\", \" \").replace(\",\", \" \") for subject in df['subject']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 100\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "f = open(os.path.join('glove.6B.100d.txt'))\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "    \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statement\n",
    "\n",
    "statement_text = ''\n",
    "\n",
    "for i in df['statement']:\n",
    "    statement_text += i\n",
    "    \n",
    "max_words = len(set(statement_text.split()))\n",
    "\n",
    "statement_texts = df['statement']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(statement_texts)\n",
    "statement_word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(statement_texts)\n",
    "statement_data = pad_sequences(sequences, maxlen = maxlen)\n",
    "\n",
    "statement_embedding_matrix = np.zeros((len(statement_word_index) + 1, output_dim))\n",
    "\n",
    "for word, i in statement_word_index.items():\n",
    "    statement_embedding_vector = embeddings_index.get(word)\n",
    "    if statement_embedding_vector is not None:\n",
    "        statement_embedding_matrix[i] = statement_embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker\n",
    "\n",
    "speaker_text = ''\n",
    "\n",
    "for i in df['speaker']:\n",
    "    speaker_text += i\n",
    "    \n",
    "max_words = len(set(speaker_text.split()))\n",
    "\n",
    "speaker_texts = df['speaker']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(speaker_texts)\n",
    "speaker_word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(speaker_texts)\n",
    "speaker_data = pad_sequences(sequences, maxlen = maxlen)\n",
    "\n",
    "speaker_embedding_matrix = np.zeros((len(speaker_word_index) + 1, output_dim))\n",
    "\n",
    "for word, i in speaker_word_index.items():\n",
    "    speaker_embedding_vector = embeddings_index.get(word)\n",
    "    if speaker_embedding_vector is not None:\n",
    "        speaker_embedding_matrix[i] = speaker_embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject\n",
    "\n",
    "subject_text = ''\n",
    "\n",
    "for i in df['subject']:\n",
    "    subject_text += i\n",
    "    \n",
    "max_words = len(set(subject_text.split()))\n",
    "\n",
    "subject_texts = df['subject']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(subject_texts)\n",
    "subject_word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(subject_texts)\n",
    "subject_data = pad_sequences(sequences, maxlen = maxlen)\n",
    "\n",
    "subject_embedding_matrix = np.zeros((len(subject_word_index) + 1, output_dim))\n",
    "\n",
    "for word, i in subject_word_index.items():\n",
    "    subject_embedding_vector = embeddings_index.get(word)\n",
    "    if subject_embedding_vector is not None:\n",
    "        subject_embedding_matrix[i] = subject_embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context\n",
    "\n",
    "context_text = ''\n",
    "\n",
    "for i in df['context']:\n",
    "    context_text += i\n",
    "    \n",
    "max_words = len(set(context_text.split()))\n",
    "\n",
    "context_texts = df['context']\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(context_texts)\n",
    "context_word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(context_texts)\n",
    "context_data = pad_sequences(sequences, maxlen = maxlen)\n",
    "\n",
    "context_embedding_matrix = np.zeros((len(context_word_index) + 1, output_dim))\n",
    "\n",
    "for word, i in context_word_index.items():\n",
    "    context_embedding_vector = embeddings_index.get(word)\n",
    "    if context_embedding_vector is not None:\n",
    "        context_embedding_matrix[i] = context_embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolution\n",
    "filters = 4\n",
    "kernel_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm\n",
    "units_lstm = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense\n",
    "units_dense = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "epochs = 2\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "statement_input = keras.Input(shape = (None,), name = \"statement_input\")\n",
    "\n",
    "# embedding\n",
    "statement_features = layers.Embedding(len(statement_word_index) + 1, output_dim = output_dim, weights = [statement_embedding_matrix], trainable = False,name = \"statement_embedding\")(statement_input)\n",
    "\n",
    "# convolution\n",
    "statement_features = layers.Conv1D(filters, kernel_size, strides = 1, padding = 'same', activation='relu', name = \"statement_convolution\")(statement_features)\n",
    "    \n",
    "# maxpooling\n",
    "statement_features = layers.MaxPooling1D(strides = 1, padding = 'same', name = \"statement_maxpooling\")(statement_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "speaker_input = keras.Input(shape = (None,), name = \"speaker_input\")\n",
    "\n",
    "# embedding\n",
    "speaker_features = layers.Embedding(len(speaker_word_index) + 1, output_dim = output_dim, weights = [speaker_embedding_matrix], trainable = False, name = \"speaker_embedding\")(speaker_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subject layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "subject_input = keras.Input(shape = (None,), name = \"subject_input\")\n",
    "\n",
    "# embedding\n",
    "subject_features = layers.Embedding(len(subject_word_index) + 1, output_dim = output_dim, weights = [subject_embedding_matrix], trainable = False, name = \"subject_embedding\")(subject_input)\n",
    "\n",
    "# Bi-LSTM\n",
    "subject_features = layers.Bidirectional(layers.LSTM(units_lstm, name = \"statement_bi-lstm\", return_sequences = True))(subject_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "context_input = keras.Input(shape = (None,), name = \"context_input\")\n",
    "\n",
    "# embedding\n",
    "context_features = layers.Embedding(len(context_word_index) + 1, output_dim = output_dim, weights = [context_embedding_matrix], trainable = False, name = \"context_embedding\")(context_input)\n",
    "\n",
    "# Bi-LSTM\n",
    "context_features = layers.Bidirectional(layers.LSTM(units_lstm, name = \"context_bi-lstm\", return_sequences = True))(context_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "merged = layers.concatenate([statement_features, speaker_features, subject_features, context_features], name = \"merge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected layer\n",
    "fully_connetcted = layers.Dense(units_dense, activation='relu', name = \"fully_connected\")(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax layer\n",
    "sigmoid = layers.Dense(2, activation='sigmoid', name = \"sigmoid\")(fully_connetcted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "model = keras.Model(\n",
    "    inputs = [statement_input, speaker_input, subject_input, context_input],\n",
    "    outputs = [sigmoid],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show model\n",
    "keras.utils.plot_model(model, \"multi_input_model.png\", show_shapes = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile and run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer = 'sgd', loss = keras.losses.BinaryCrossentropy(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainingsdata\n",
    "statement_data_train = statement_data[:int(12820*0.7)]\n",
    "speaker_data_train = speaker_data[:int(12820*0.7)]\n",
    "subject_data_train = subject_data[:int(12820*0.7)]\n",
    "context_data_train = context_data[:int(12820*0.7)]\n",
    "label_data_train = df['label'][:int(12820*0.7)]\n",
    "#label_data_train = to_categorical(label_data_train)\n",
    "\n",
    "# test data\n",
    "statement_data_test = statement_data[int(12820*0.7):]\n",
    "speaker_data_test = speaker_data[int(12820*0.7):]\n",
    "subject_data_test = subject_data[int(12820*0.7):]\n",
    "context_data_test = context_data[int(12820*0.7):]\n",
    "label_data_test = df['label'][int(12820*0.7):]\n",
    "#label_data_test = to_categorical(label_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    {\"statement_input\": statement_data_train,\n",
    "    \"speaker_input\": speaker_data_train,\n",
    "     \"subject_input\": subject_data_train,\n",
    "     \"context_input\": context_data_train},\n",
    "    {'sigmoid': label_data_train},\n",
    "    validation_data = (\n",
    "        [statement_data_test, speaker_data_test, subject_data_test, context_data_test], \n",
    "        label_data_test),\n",
    "    epochs = epochs,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
